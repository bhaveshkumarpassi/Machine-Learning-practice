My Approach and intuitions about Pet Adoption Machine learning Challenge.

So, as i started working on the dataset i started drawing intuitions and observations about the training data using numpy and pandas statistical and matplotlib and seaborn's visualisation support in pthon environment.

Major Preprocessing ans Feature engineering :

 As a first thing i checked the class distribution of our target variables and i found that their were great class imbalance in both of the target variables to handle this in a later stage i.e, after major preprocessing i used imblearn's SMOTETomek's method to upsample the minority classes. so i started with basic preprocessing of training data which is essential before training any machine learning model and as a first thing i found that there were not much outliers error in dataset through seaborn's box plot's and other visualisations which is a good thing. then i looked up for any missing values and there i found that condition feature which contained too many missing values. Now to impute them i used many techniques like KNN imputation , clustering imputation and i also engineered a new feature which basically contained diference between listing date and issue date in days i thought it might give an idea about the condition of pet. i also tried to impute it with help of X2 feature given but none of the ideas at the end performed well so after many experimentation i noticed a weird relation and it was that the condition column values were only missing when for that train example the breed_category was 2. so that boosted my confidence to let me make that Nan as also a class named 3 for better prediction of atleast breed_category (which was vey cruicial because as the design of problem was multi-target predications so as an approach i had a mindset that i will first impute breed_category and then considering it also as an independent variable (feature) i will make prediction of pet_category) and at the end it actually did worked so this was my final imputation for any missing values both in train and test set given. Then through some visualisations and experimentation i found that some values of length feature were 0 which shouldn't be the case so i replaced the zeroes with the average of the length column both in train and test set. As far as some more feature creation is concerned i also created two more feature one was simply containing the length by height ratio and other was extracted from the pet_id because i noticed in the id's pattern that the every sixth character was just couple of discrete values rather than any of the numbers so i created that as another categorical feature but at the end after all training and testing these features also didn't helped me much so i dropped them as well. okay, so then when i noticed that the color_type feature was just conatined to many categorical strings which might have created a problem because to handle this categorical feature i was to onehot encode them and having just to many of onehot encode feature might have reduced the efficiency of my algorithm so as per the frequency of colors in dataset i noticed that their were several colors which were very rarely present in dataset so i clubed such relatively rare color types into another category called 'rare' and then i performed onehot encoding using pandas method of get dummies. so this was pretty much in short all about me exploring the preprocessing of the training set.

 Deciding The Algorithm and training:

Then another cruicial step comes that was to decide which algorithm to use so after observing  and visualisating the relationships between different variables the linear or any polynomial separation of classes doesn't seemed possible. and also according to the size and shape of the training set i was more inclined towards tree based classifications and this decision was also influenced by the fact that data contained just too many categorical features so i decided to apply an algorithm which was robust to all above mentioned characterstics and is Random forest classifier from sklearn's library which is based on bootsrap method of implementing Random forests for clasifiaction. so then i trained my model on the upsampled version of data which i created to handle class imbalance problem. 

evaluation:

So as far as evaluation of my trained model was concerned i did evaluation on a train and dev set of 80 isto 20 ratio basis and then evaluated the f1_scores form metrics available from sklearn i also applied cross_val_score method (based on startified k fold method by default) from sklearn to get better intuition about how my models performed. Then i also performed explicit Stratified k fold thechnique from sklearn with a k fold of value equals 5 i.e i basically trained 5 different Random forest classifiers on the training set and then evaluated the result and then stored these trained models. (I had also tried hyperparamter tunning of my models through sklearn's GridSearch method to improve my models performance but at the end it didn't helped much so i simply trained the things on default hyperparameters.).

Predictions on unseen Data (Test file):

for making prdictions on the test set i first of all did all the preprocessing required in a similar way as i did for training set and then i made preications on each of the k fold models which i trained on training set and then stored the most commonly choosed target label from each of the k fold models to enhance the chances of predicted target labels beign correct, and after that i just created my final dataframe containing pet_id , prdicted breed_category and pet_category for test data and then read that in csv file through pandas methods.


Note: The ipython notebook which i am submitting showing my work did not contain each and every step i mentioned above as i said many of them were for experimentation so i deleted those as and when i found that they were not much helpful to maintain the cleanliness of my work.